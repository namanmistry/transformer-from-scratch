# Implementation of Transformer Encoder from Scratch using TensorFlow

This project provides a complete implementation of the Transformer Encoder architecture using TensorFlow. The Transformer Encoder is a key component of the Transformer model, which has revolutionized natural language processing tasks such as machine translation and text generation.

## Prerequisites

Make sure you have the following libraries installed before running the project:

- TensorFlow: You can install it using the command `pip install tensorflow`.
- NumPy: You can install it using the command `pip install numpy`.

## Usage

To run the code, follow these steps:

1. Clone the repository to your local machine.
2. Install the required libraries specified in the `requirements.txt` file.
3. Open the `untitled.ipynb` file in Jupyter Notebook or any Python IDE.
4. Run the code cells in sequential order.
5. The notebook will train the Transformer Encoder on a sample dataset and display the training loss and accuracy metrics.

Feel free to modify the code and experiment with different hyperparameters or datasets to further explore the capabilities of the Transformer Encoder.

## Results

After extensive implementation and training, the Transformer Encoder architecture was successfully implemented from scratch using TensorFlow. 

By leveraging the power of the Transformer Encoder, this project opens up opportunities for various natural language processing applications, including language translation, text summarization, and sentiment analysis.

Get started with the code and unlock the potential of the Transformer Encoder for your own projects!
